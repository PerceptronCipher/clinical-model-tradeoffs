{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5a73bc",
   "metadata": {},
   "source": [
    "# Section 2: Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45c0e9",
   "metadata": {},
   "source": [
    "## Purpose of Preprocessing\n",
    "\n",
    "This section documents the preprocessing decisions applied to the dataset prior to model training. The goal is not to maximize model performance, but to ensure a fair and consistent comparison between different classification algorithms.\n",
    "\n",
    "All preprocessing steps are designed to reflect realistic data preparation practices commonly encountered in healthcare-related datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d53718",
   "metadata": {},
   "source": [
    "## Target Variable Binarization\n",
    "\n",
    "The original target variable encodes multiple levels of heart disease severity. For this project, the target is converted into a binary variable:\n",
    "\n",
    "- 0 → No heart disease\n",
    "- 1 → Presence of heart disease\n",
    "\n",
    "This formulation reflects a clinically relevant screening task where the primary objective is to identify whether heart disease is present, rather than estimating severity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e7ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important libraries\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf331c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# data (pandas dataframes)\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b109396",
   "metadata": {},
   "source": [
    "### The dataset is reloaded here to keep preprocessing self-contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ad900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        299 non-null    float64\n",
      " 12  thal      301 non-null    float64\n",
      "dtypes: float64(3), int64(10)\n",
      "memory usage: 30.9 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   num     303 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 2.5 KB\n",
      "Dataset Information: \n",
      " None\n",
      "None \n",
      "Dataset Shape: \n",
      "(303, 13)\n",
      " (303, 1)\n"
     ]
    }
   ],
   "source": [
    "# print dataset information\n",
    "print(f\"Dataset Information: \\n {X.info()}\\n{y.info()} \\nDataset Shape: \\n{X.shape}\\n {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06378ed0",
   "metadata": {},
   "source": [
    "### Converting Multi-class to Binary Classification\n",
    "\n",
    "The original target variable has 5 classes (0-4):\n",
    "- Class 0: No heart disease\n",
    "- Classes 1-4: Varying levels of heart disease severity\n",
    "\n",
    "We're converting this to binary classification:\n",
    "- 0 → No disease\n",
    "- 1 → Disease present (combining all severity levels 1-4)\n",
    "\n",
    "This simplifies the problem and balances the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015a434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n",
      "0      164\n",
      "1      139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert target to binary\n",
    "y = (y > 0).astype(int)\n",
    "\n",
    "# Check new distribution\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889922da",
   "metadata": {},
   "source": [
    "### Missing Value Handling\n",
    "\n",
    "After converting to binary classification, we need to address missing values in the dataset.\n",
    "\n",
    "The features `ca` (number of major vessels) and `thal` (thalassemia) contain missing values. Both features are clinically relevant for heart disease prediction, so we cannot drop them.\n",
    "\n",
    "We'll use a simple, transparent imputation strategy: filling missing values with the median for each feature. This preserves the dataset size and maintains the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f378872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per feature:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          4\n",
      "thal        2\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 6\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per feature:\")\n",
    "print(X.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a10289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# convert back to DataFrame to preserve column names\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ccc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "0\n",
      "\n",
      "Dataset size preserved: (303, 13)\n"
     ]
    }
   ],
   "source": [
    "# verify no missing values remain\n",
    "print(\"Missing values after imputation:\")\n",
    "print(X.isnull().sum().sum())\n",
    "print(f\"\\nDataset size preserved: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971a401",
   "metadata": {},
   "source": [
    "### Why median and not with mode or mean?\n",
    "\n",
    "- median is more robust to outliers as we do not want outliers interference in this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28731ba2",
   "metadata": {},
   "source": [
    "### Feature Type Identification\n",
    "\n",
    "Before scaling or encoding, we need to identify which features are numerical and which are categorical.\n",
    "\n",
    "Numerical features are continuous values (age, blood pressure, cholesterol) that can be scaled.\n",
    "\n",
    "Categorical features are discrete categories (sex, chest pain type, thalassemia) that will need encoding later.\n",
    "\n",
    "Understanding feature types is important because:\n",
    "- Numerical features benefit from scaling\n",
    "- Categorical features need encoding (one-hot, label, frequency, target encoding)\n",
    "- Different feature types behave differently in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b341a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features \n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_features = X.select_dtypes(include=[\"int64\", 'float64']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14805976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORICAL FEATURES: \n",
      " [] \n",
      " NUMERICAL FEATURES: \n",
      " ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n"
     ]
    }
   ],
   "source": [
    "# view separated features \n",
    "print(f\"CATEGORICAL FEATURES: \\n {categorical_features.tolist()} \\n NUMERICAL FEATURES: \\n {numerical_features.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bd7eb",
   "metadata": {},
   "source": [
    "### Outcome \n",
    "- As shown, all features are numerical either in integer or float\n",
    "- No object data types hence encoding is not necessary for categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19088a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Numerical Feature Summary: \n",
      "\n",
      "              age         sex          cp    trestbps        chol         fbs  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
      "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
      "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
      "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
      "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
      "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
      "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.663366   \n",
      "std      0.994971   22.875003    0.469794    1.161075    0.616226    0.934375   \n",
      "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000   \n",
      "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
      "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000   \n",
      "75%      2.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
      "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
      "\n",
      "             thal  \n",
      "count  303.000000  \n",
      "mean     4.722772  \n",
      "std      1.938383  \n",
      "min      3.000000  \n",
      "25%      3.000000  \n",
      "50%      3.000000  \n",
      "75%      7.000000  \n",
      "max      7.000000  \n"
     ]
    }
   ],
   "source": [
    "# Display summary of numerical features\n",
    "print(\"\\n Numerical Feature Summary: \\n\")\n",
    "print(X[numerical_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d78979",
   "metadata": {},
   "source": [
    "### Feature Type Conclusion\n",
    "\n",
    "\n",
    "- Numerical features might need scaling (standardization) or even normalization\n",
    "- Categorical features will need encoding (one-hot encoding or label-encoding or frequency-encoding or target-encoding) depending on what's best for such features\n",
    "- Different preprocessing steps apply to each type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9896b",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Scaling is a critical preprocessing step that affects model performance differently by dominating the euclidean distance or variance:\n",
    "\n",
    "**Why scaling matters in this project?:**\n",
    "- KNN relies on distance calculations, so features on different scales heavily impact results\n",
    "- SVM (Support Vector Machine) is also distance-based and requires scaled features for optimal performance\n",
    "- Logistic Regression is less sensitive to scaling but can still benefit from it\n",
    "\n",
    "**Our approach:**\n",
    "We'll treat scaling as an experimental factor in this project. We'll scale the numerical features using StandardScaler (mean=0, std=1) + MinMax Scaling and compare model performance with one, with both and without scaling.\n",
    "\n",
    "**Important:** We only scale the features (X), never the target (y). We must also avoid data leakage by fitting the scaler only on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56ef42",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
